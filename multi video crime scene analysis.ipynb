{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d8aa9e2f-88e0-4711-a424-6e509fa667d2",
   "metadata": {},
   "source": [
    "This script is used to extract individual frames from a video file. It's important because the rest of the code processes frames individually to extract visual and textual features.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Input: A video file (video_path) and an output directory where the frames will be saved (output_dir).\n",
    "Frame Extraction: The code uses OpenCV's cv2.VideoCapture to load the video. It reads each frame in sequence.\n",
    "Save Frames: Every frame that is extracted is saved as an image (JPEG) file in the specified directory. The frame_rate argument allows you to control how often a frame is saved (e.g., every second frame, every 5th frame, etc.).\n",
    "Output: A series of image files (frame_0.jpg, frame_1.jpg, etc.) in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f52e32-bbeb-4aa1-a12f-903d1de7c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames_from_video(video_path, output_dir, frame_rate=1):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_rate == 0:\n",
    "            frame_filename = os.path.join(output_dir, f\"frame_{count}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"data/video/crime_scene_video.mp4\"\n",
    "    output_dir = \"data/frames\"\n",
    "    extract_frames_from_video(video_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "934f0916-a0c9-42d4-8bb6-d2b371515e9b",
   "metadata": {},
   "source": [
    "This script processes the extracted frames to detect and track crime-related objects (such as weapons or suspicious persons) using the YOLO object detection model and DeepSORT for tracking.\n",
    "\n",
    "Steps:\n",
    "\n",
    "YOLO Object Detection:\n",
    "The code loads the YOLOv5 model via torch.hub.load('ultralytics/yolov5', 'yolov5s'). YOLO is a deep learning model for real-time object detection.\n",
    "It processes each frame to detect objects and returns a list of detected objects along with their bounding boxes (coordinates of the objects in the image).\n",
    "It filters out objects that are relevant to crime scene analysis (e.g., knife, gun, person).\n",
    "DeepSORT Tracking:\n",
    "The DeepSORT tracker is initialized to track the detected objects over multiple frames. It uses the bounding box coordinates and confidence scores from YOLO to keep track of objects across frames.\n",
    "Output: For each frame, the detected objects are tracked, and their IDs are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb5109-b67c-4e29-aae2-144c758cc925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from PIL import Image\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLO model\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)  # Initialize DeepSORT tracker\n",
    "\n",
    "def extract_and_track_objects(frame_path):\n",
    "    img = Image.open(frame_path).convert('RGB')\n",
    "    results = yolo_model(img)\n",
    "    detections = results.pandas().xyxy[0]\n",
    "    \n",
    "    # Filter crime-related objects (e.g., knife, gun, person)\n",
    "    crime_objects = detections[detections['name'].isin(['knife', 'gun', 'person'])]\n",
    "    \n",
    "    # Perform tracking using DeepSORT\n",
    "    bboxes = crime_objects[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "    confidences = crime_objects['confidence'].values\n",
    "    track_ids = tracker.update_tracks(bboxes, confidences=confidences, frame=img)\n",
    "    \n",
    "    return track_ids\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    frame_path = \"data/frames/frame_0.jpg\"\n",
    "    track_ids = extract_and_track_objects(frame_path)\n",
    "    print(\"Tracked Objects:\", track_ids)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "584a5176-61e6-4495-8376-5f746c588b99",
   "metadata": {},
   "source": [
    "This script extracts textual information from the frames using Optical Character Recognition (OCR) with Tesseract. It filters the text for crime-related keywords.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Tesseract OCR:\n",
    "The pytesseract.image_to_string() function is used to extract any text present in the image (frame).\n",
    "Filter Crime-Related Text:\n",
    "The extracted text is checked against a predefined list of crime-related keywords (such as gun, help, shoot, etc.). If any of these keywords are present, it indicates that the text might be related to a crime.\n",
    "Output: A list of crime-related keywords or phrases found in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a839f8-4b11-4011-a317-88558e1a2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "crime_keywords = [\"gun\", \"help\", \"shoot\", \"suspect\", \"robbery\", \"assault\", \"scream\", \"emergency\"]\n",
    "\n",
    "def extract_and_filter_text(frame_path):\n",
    "    img = cv2.imread(frame_path)\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    \n",
    "    # Check if any crime-related words exist in the extracted text\n",
    "    crime_related_text = [word for word in crime_keywords if word.lower() in text.lower()]\n",
    "    return crime_related_text\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    frame_path = \"data/frames/frame_0.jpg\"\n",
    "    crime_text = extract_and_filter_text(frame_path)\n",
    "    print(\"Crime-Related Text:\", crime_text)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cbd0b98-64b1-4c8e-835d-bdb2dad963cc",
   "metadata": {},
   "source": [
    "This script takes multiple video files, processes each video by extracting frames, analyzing them for visual and textual features, and then combines the processed frames into a single output video.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Input: A list of video files (video_paths), and an output video path (output_video_path).\n",
    "Extract Frames: For each video, the frames are extracted using the extract_frames_from_video function from extract_frames.py.\n",
    "Process Each Frame:\n",
    "For each extracted frame, visual features (detected objects and tracking information) are obtained using extract_and_track_objects from extract_visual_features.py.\n",
    "The textual features are extracted using extract_and_filter_text from extract_textual_features.py.\n",
    "Visualize:\n",
    "The frames are annotated with bounding boxes around tracked objects and labeled with their respective track IDs.\n",
    "Crime-related text is also added to the frame, if detected.\n",
    "Write to Output Video:\n",
    "The processed frames are written to a new video file (output_video_path) using OpenCV's cv2.VideoWriter. This combines the frames into a single output video.\n",
    "Output: A single video file (output_video_path) that contains all the processed frames, annotations, and detected text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4be033-5bdd-4d1e-bfaa-dfd01587a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from extract_frames import extract_frames_from_video\n",
    "from extract_visual_features import extract_and_track_objects\n",
    "from extract_textual_features import extract_and_filter_text\n",
    "\n",
    "\n",
    "def combine_videos_and_generate_output(video_paths, output_video_path, frame_rate=1, resolution=(640, 480)):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Video codec\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 30.0, resolution)  # Create video writer\n",
    "\n",
    "    for video_path in video_paths:\n",
    "        output_dir = \"data/frames\"  # Directory to save frames from the video\n",
    "        extract_frames_from_video(video_path, output_dir, frame_rate)\n",
    "        \n",
    "        for frame_file in sorted(os.listdir(output_dir)):\n",
    "            frame_path = os.path.join(output_dir, frame_file)\n",
    "            \n",
    "            # Process each frame for tracking and text extraction\n",
    "            track_ids = extract_and_track_objects(frame_path)\n",
    "            crime_text = extract_and_filter_text(frame_path)\n",
    "            \n",
    "            # Visualize the frame with tracking information\n",
    "            frame = cv2.imread(frame_path)\n",
    "            \n",
    "            # Draw bounding boxes and track IDs on the frame\n",
    "            for track in track_ids:\n",
    "                x1, y1, x2, y2, track_id = track\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f\"ID:{int(track_id)}\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "            \n",
    "            # Add crime-related text info\n",
    "            if crime_text:\n",
    "                cv2.putText(frame, ' '.join(crime_text), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            # Write the processed frame to the output video\n",
    "            out.write(frame)\n",
    "\n",
    "    out.release()  # Release the video writer\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    video_paths = [\"data/video/crime_scene_video_1.mp4\", \"data/video/crime_scene_video_2.mp4\"]\n",
    "    output_video_path = \"data/output/crime_scene_analysis_output.avi\"\n",
    "    combine_videos_and_generate_output(video_paths, output_video_path, frame_rate=5)\n",
    "    print(\"Output video saved as:\", output_video_path)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1e0d2b7-ec6e-4fc9-a846-78f234e4efaf",
   "metadata": {},
   "source": [
    "Key Components of the Workflow\n",
    "Video Frame Extraction:\n",
    "\n",
    "The frames from each video are extracted and saved as individual image files. This allows the subsequent steps to process each frame separately.\n",
    "Object Detection with YOLO:\n",
    "\n",
    "YOLO is used for detecting objects of interest (like weapons, persons, etc.) within each frame. This helps in identifying crime-related activities visually.\n",
    "Tracking with DeepSORT:\n",
    "\n",
    "Once an object is detected by YOLO, DeepSORT is used to assign a unique ID to each object and track its movement across multiple frames. This helps in keeping track of objects across a scene.\n",
    "Textual Analysis with Tesseract OCR:\n",
    "\n",
    "Text that might appear in the video (e.g., signs, license plates, text on screens) is extracted using OCR and checked for any crime-related keywords.\n",
    "Combining Multiple Videos:\n",
    "\n",
    "Multiple video files are processed one by one, and all their frames are combined into a single output video, making it easy to analyze and visualize the results from different sources in one video."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
